<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nzxhuong'log (Posts about importance sampling)</title><link>https://nzxhuong.github.io/</link><description></description><atom:link href="https://nzxhuong.github.io/categories/importance-sampling.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Wed, 02 Apr 2025 14:13:09 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Prioritized Experience Replay and Importance Sampling</title><link>https://nzxhuong.github.io/posts/prioritized-experience-replay-and-importance-sampling/</link><dc:creator>Ngo Truong</dc:creator><description>&lt;div&gt;&lt;p&gt;Before diving into importance sampling and prioritized experience, we should be on the same page. In Deep Reinforcement Learning, especially in algorithms like Deep Q-Networks (DQN), we often use an Experience Replay buffer. This buffer stores past transitions experienced by the agent, typically tuples of (State &lt;span class="math"&gt;\(S\)&lt;/span&gt;, Action &lt;span class="math"&gt;\(A\)&lt;/span&gt;, Reward &lt;span class="math"&gt;\(R\)&lt;/span&gt;, Next State &lt;span class="math"&gt;\(S'\)&lt;/span&gt;, Done flag &lt;span class="math"&gt;\(d\)&lt;/span&gt;). During training, we sample mini-batches of these transitions from the buffer to update the agent's policy or value function.&lt;/p&gt;
&lt;p&gt;The standard approach is to sample transitions &lt;em&gt;uniformly&lt;/em&gt; at random from the replay buffer. If the buffer contains &lt;span class="math"&gt;\(N\)&lt;/span&gt; transitions, each transition has a probability of &lt;span class="math"&gt;\(1/N\)&lt;/span&gt; (or &lt;span class="math"&gt;\(b/N\)&lt;/span&gt; for a mini-batch of size &lt;span class="math"&gt;\(b\)&lt;/span&gt;) of being selected. While simple and effective in breaking correlations between consecutive experiences, uniform sampling treats all transitions as equally important. However, some transitions might be much more informative or surprising to the agent than others, potentially offering a greater learning opportunity. Uniform sampling means these crucial transitions might be sampled rarely, especially in large buffers, potentially slowing down learning. This post explores Prioritized Experience Replay (PER), a technique designed to address this by sampling important transitions more frequently, using the mathematical framework of Importance Sampling.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://nzxhuong.github.io/posts/prioritized-experience-replay-and-importance-sampling/"&gt;Read moreâ€¦&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>importance sampling</category><category>mathjax</category><category>prioritized experience replay</category><category>reinforcement learning</category><guid>https://nzxhuong.github.io/posts/prioritized-experience-replay-and-importance-sampling/</guid><pubDate>Wed, 02 Apr 2025 13:31:56 GMT</pubDate></item></channel></rss>