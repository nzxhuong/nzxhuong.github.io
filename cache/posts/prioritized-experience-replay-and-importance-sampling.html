<p>Before diving into importance sampling and prioritized experience, we should be on the same page. In Deep Reinforcement Learning, especially in algorithms like Deep Q-Networks (DQN), we often use an Experience Replay buffer. This buffer stores past transitions experienced by the agent, typically tuples of (State <span class="math">\(S\)</span>, Action <span class="math">\(A\)</span>, Reward <span class="math">\(R\)</span>, Next State <span class="math">\(S'\)</span>, Done flag <span class="math">\(d\)</span>). During training, we sample mini-batches of these transitions from the buffer to update the agent's policy or value function.</p>
<p>The standard approach is to sample transitions <em>uniformly</em> at random from the replay buffer. If the buffer contains <span class="math">\(N\)</span> transitions, each transition has a probability of <span class="math">\(1/N\)</span> (or <span class="math">\(b/N\)</span> for a mini-batch of size <span class="math">\(b\)</span>) of being selected. While simple and effective in breaking correlations between consecutive experiences, uniform sampling treats all transitions as equally important. However, some transitions might be much more informative or surprising to the agent than others, potentially offering a greater learning opportunity. Uniform sampling means these crucial transitions might be sampled rarely, especially in large buffers, potentially slowing down learning. This post explores Prioritized Experience Replay (PER), a technique designed to address this by sampling important transitions more frequently, using the mathematical framework of Importance Sampling.</p>
<!-- TEASER_END -->
<p>As mentioned, the replay buffer stores transitions <span class="math">\((S_t, A_t, R_{t+1}, S_{t+1}, d_{t+1})\)</span>. Uniform sampling gives every stored experience an equal chance of being used for learning updates. Consider a scenario where the agent encounters a rare but highly informative event (e.g., receiving an unexpectedly large reward or penalty). Such transitions can significantly impact the agent's understanding of the environment. However, if the buffer holds thousands or millions of transitions, these critical experiences might only be sampled occasionally under a uniform strategy, hindering efficient learning. We want a way to focus the learning process on the transitions that offer the most potential for improvement.</p>
<p>Importance Sampling is a technique that allows us to estimate the expectation of a function with respect to one probability distribution (<span class="math">\(p(x)\)</span>), while drawing samples from a <em>different</em> probability distribution (<span class="math">\(q(x)\)</span>).</p>
<p>Suppose we want to compute the expected value of a function <span class="math">\(f(x)\)</span> where <span class="math">\(x\)</span> is drawn from distribution <span class="math">\(p(x)\)</span>:</p>
<div class="math">
\begin{equation*}
E_{x \sim p}[f(x)] = \sum_x p(x)f(x) \quad \text{(for discrete x)}
\end{equation*}
</div>
<p>If we have another distribution <span class="math">\(q(x)\)</span> defined over the same domain, such that <span class="math">\(q(x) &gt; 0\)</span> whenever <span class="math">\(p(x)f(x) \neq 0\)</span>, we can rewrite the expectation:</p>
<div class="math">
\begin{align*}
E_{x \sim p}[f(x)] &amp;= \sum_x p(x)f(x) \\
                  &amp;= \sum_x \frac{q(x)}{q(x)} p(x)f(x) \\
                  &amp;= \sum_x q(x) \frac{p(x)}{q(x)} f(x) \\
                  &amp;= E_{x \sim q}\left[ \frac{p(x)}{q(x)} f(x) \right]
\end{align*}
</div>
<p>The term <span class="math">\(w(x) = \frac{p(x)}{q(x)}\)</span> is called the <strong>importance weight</strong>. It corrects for the bias introduced by sampling from <span class="math">\(q(x)\)</span> instead of <span class="math">\(p(x)\)</span>.</p>
<p>We can estimate these expectations using samples from <span class="math">\(p\)</span> or <span class="math">\(q\)</span>:</p>
<ul class="simple">
<li><p>Sampling from <span class="math">\(p\)</span>: <span class="math">\(E_{x \sim p}[f(x)] \approx \frac{1}{n} \sum_{i=1}^n f(x_i)\)</span>, where <span class="math">\(x_i \sim p\)</span>.</p></li>
<li><p>Sampling from <span class="math">\(q\)</span>: <span class="math">\(E_{x \sim p}[f(x)] \approx \frac{1}{n} \sum_{i=1}^n \frac{p(x_i)}{q(x_i)} f(x_i) = \frac{1}{n} \sum_{i=1}^n w(x_i) f(x_i)\)</span>, where <span class="math">\(x_i \sim q\)</span>.</p></li>
</ul>
<p>This gives us a tool to sample transitions non-uniformly (<span class="math">\(q\)</span>) while still correctly estimating the objective function defined under uniform sampling (<span class="math">\(p\)</span>). The next step is to define a suitable non-uniform distribution <span class="math">\(q\)</span> for our replay buffer.</p>
<p>PER aims to sample transitions from the replay buffer based on their &quot;importance&quot; or &quot;priority&quot;. A common measure of importance is the magnitude of the <strong>Temporal Difference (TD) error</strong>, which reflects how &quot;surprising&quot; a transition is to the current value function estimate. For a transition <span class="math">\(i = (s, a, r, s')\)</span>, the TD error <span class="math">\(\delta_i\)</span> in DQN is:</p>
<div class="math">
\begin{equation*}
\delta_i = r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta)
\end{equation*}
</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math">\(Q(s, a; \theta)\)</span> is the Q-value estimated by the main network with parameters <span class="math">\(\theta\)</span>.</p></li>
<li><p><span class="math">\(Q(s', a'; \theta^{-})\)</span> is the Q-value estimated by the target network with parameters <span class="math">\(\theta^{-}\)</span>.</p></li>
<li><p><span class="math">\(\gamma\)</span> is the discount factor.</p></li>
<li><p>A large absolute TD error <span class="math">\(|\delta_i|\)</span> suggests that the Q-value prediction for <span class="math">\((s, a)\)</span> was inaccurate and the transition offers a significant learning opportunity.</p></li>
</ul>
<p>If you unfamiliar with all the terms above, i'm highly recommend you to watch the video of <a class="reference external" href="https://www.youtube.com/watch?v=UoPei5o4fps">Value Function Approximation</a> by David Silver, great video.</p>
<p>Given the TD error <span class="math">\(|\delta_i|\)</span> for each transition <span class="math">\(i\)</span> in the buffer, we need to convert these errors into sampling probabilities <span class="math">\(q(i)\)</span>. Two common methods proposed by DeepMind are:</p>
<ol class="arabic">
<li><p><strong>Proportional Prioritization:</strong> The priority <span class="math">\(p_i\)</span> of transition <span class="math">\(i\)</span> is directly related to its TD error:</p>
<div class="math">
\begin{equation*}
p_i = |\delta_i| + \epsilon
\end{equation*}
</div>
<p>where <span class="math">\(\epsilon\)</span> is a small positive constant to ensure that all transitions have a non-zero priority.</p>
</li>
<li><p><strong>Rank-Based Prioritization:</strong> Priorities are assigned based on the rank of the transition when sorted by <span class="math">\(|\delta_i|\)</span> in descending order:</p>
<div class="math">
\begin{equation*}
p_i = \frac{1}{\text{rank}(i)}
\end{equation*}
</div>
<p>where <span class="math">\(\text{rank}(i)\)</span> is the rank of transition <span class="math">\(i\)</span> (starting from 1 for the highest <span class="math">\(|\delta_i|\)</span>). This method is less sensitive to outliers in TD error magnitudes compared to the proportional method.</p>
</li>
</ol>
<p>Once priorities <span class="math">\(p_i\)</span> are assigned to all <span class="math">\(N\)</span> transitions in the buffer, the probability <span class="math">\(q(i)\)</span> of sampling transition <span class="math">\(i\)</span> is calculated by normalizing the priorities:</p>
<div class="math">
\begin{equation*}
q(i) = \frac{p_i}{\sum_{j=1}^N p_j}
\end{equation*}
</div>
<p>Sampling is typically done using these probabilities, often implemented efficiently using data structures like SumTrees.</p>
<p>Now we put everything together. The standard DQN objective minimizes the expected squared TD error, assuming uniform sampling (<span class="math">\(p(i) = 1/N\)</span>):</p>
<div class="math">
\begin{equation*}
L(\theta) = E_{(s,a,r,s') \sim U(D)} [ \delta_i^2 ] = E_{i \sim p} [ \delta_i^2 ]
\end{equation*}
</div>
<p>When sampling with non-uniform probabilities <span class="math">\(q(i)\)</span> from PER, we must use importance sampling weights to correct the bias. The loss function becomes:</p>
<div class="math">
\begin{equation*}
L(\theta) = E_{i \sim q} [ w_i \delta_i^2 ]
\end{equation*}
</div>
<p>The importance weight <span class="math">\(w_i\)</span> for transition <span class="math">\(i\)</span> is:</p>
<div class="math">
\begin{equation*}
w_i = \frac{p(i)}{q(i)} = \frac{1/N}{q(i)} = \frac{1}{N \cdot q(i)}
\end{equation*}
</div>
<p>The gradient update for the DQN parameters <span class="math">\(\theta\)</span> using a mini-batch of size <span class="math">\(b\)</span> sampled according to <span class="math">\(q(i)\)</span> is then:</p>
<div class="math">
\begin{equation*}
\Delta \theta = \frac{1}{b} \sum_{i \in \text{batch}} w_i \delta_i \nabla_\theta Q(s_i, a_i; \theta)
\end{equation*}
</div>
<p>We then update the parameters of the model using the gradient above with some optimizer (e.g., Adam, RMSProp). But there are a few things to note:
Some time the magnitude of TD-error can be very large, which can lead to large importance weights. This can cause instability in training. To mitigate this, we can use a parameter <span class="math">\(\alpha\)</span> to scale the importance weights where <span class="math">\(\alpha \in [0, 1]\)</span>:</p>
<div class="math">
\begin{equation*}
p_i = |\delta_i|^\alpha + \epsilon
\end{equation*}
</div>
<p>For the importance weights, we can do the same trick with a parameter <span class="math">\(\beta\)</span> to scale the importance weights where <span class="math">\(\beta \in [0, 1]\)</span>:</p>
<div class="math">
\begin{equation*}
w_i = \left( \frac{p(i)}{q(i)} \right)^\beta
\end{equation*}
</div>
<p>Prioritized Experience Replay provides a mechanism to focus learning on the most informative transitions by sampling them more frequently than uniform random sampling would dictate. By combining this prioritized sampling strategy with Importance Sampling weights, PER can significantly accelerate learning and improve the performance of Deep Reinforcement Learning agents like DQN, demonstrating the power of moving beyond simple uniform sampling in experience replay.</p>
<p>In the future, there might be a post on how to implement PER in PyTorch.</p>
<p><br/><h3> Further Reading </h3></p>
<ul class="simple">
<li><p>Original PER Paper: Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2015). Prioritized Experience Replay. <a class="reference external" href="https://arxiv.org/abs/1511.05952">arXiv:1511.05952</a>.</p></li>
<li><p>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Importance_sampling">Importance Sampling</a></p></li>
</ul>
<p><br /></p>
