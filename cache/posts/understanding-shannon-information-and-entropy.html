<p>Many materials on this topic start with Claude Shannon’s concept of information. So let’s start with that. <br />
Information, in Shannon's theory, is defined in the context of transferring a message from a source (transmitter) to a receiver over a channel. Imagine tossing a coin. In this scenario, the coin toss outcome acts as the source (transmitter), and you, observing the outcome, are the receiver.</p>
<!-- TEASER_END -->
<p>For simplicity, let's assume the coin is fair (equal probability of heads or tails), cannot land on its edge, and is tossed 10 times. We'll also assume that we toss this coin in an ideal environment (so that we will not have noise in the probability distribution of head or tail).</p>
<p>Suppose you initially believe the coin is rigged to land heads 90% of the time. So, when we finish tossing the fair coin, we observe approximately 5 heads and 5 tails. This outcome differs from our initial belief, which makes it surprising and thus, conveys more information to us. As you can imagine, if the coin were actually rigged as believed, the outcome would align with expectations and therefore convey less information.</p>
<p><br/><h3> Defining Information </h3></p>
<p>With that in mind, we can construct a function that quantifies the information of an event, let's call it <span class="math">\(I(e)\)</span>. This function should have certain desirable properties:</p>
<ul class="simple">
<li><p>It should yield a higher value when the probability of the event <span class="math">\(p\)</span> is low (more surprise) and a lower value when the probability is high (less surprise).</p></li>
<li><p>For two independent events, the information gained from observing both should be the sum of the information gained from each individually.</p></li>
</ul>
<p>A function that satisfies these properties is the logarithm of the reciprocal of the probability:</p>
<div class="math">
\begin{equation*}
I(p) = \log(1/p) = -\log(p) \tag{1}
\end{equation*}
</div>
<p>The base of the logarithm determines the units of information (e.g., base 2 gives bits, base <span class="math">\(e\)</span> gives nats).</p>
<p><br/><h3> Defining Entropy </h3></p>
<p>Entropy is simply the expected information over all possible events in a probability distribution. For a given discrete probability distribution for a random variable <span class="math">\(X\)</span> with possible outcomes <span class="math">\(x_1, x_2, \ldots, x_n\)</span> having probabilities <span class="math">\(p_1, p_2, \ldots, p_n\)</span>, we define the entropy of <span class="math">\(X\)</span>, denoted as <span class="math">\(H[X]\)</span>.</p>
<div class="math">
\begin{align*}
H[X] &amp;= E[I(X)] \\
     &amp;= \sum_{i=1}^n P(x_i)I(x_i) \quad \text{(definition of expected value)} \\
     &amp;= \sum_{i=1}^n p_i \log(1/p_i) \\
     &amp;= -\sum_{i=1}^n p_i \log(p_i) \quad \text{(since } \log(1/p_i) = -\log(p_i) \text{)} \tag{2}
\end{align*}
</div>
<p>Note: By convention, <span class="math">\(0 \log(0) = 0\)</span>.</p>
<p>Entropy, then, measures the average unpredictability (or average surprise) of a random variable's outcomes. Higher entropy signifies greater uncertainty, making the outcomes harder to predict on average.</p>
